extern "C" __global__ __launch_bounds__(256,2) // hint: max 256 threads, min 2 blocks/SM
void warp_reduce_kernel(const float * __restrict__ a, float * __restrict__ out, int N) {
  __shared__ float s[32]; // one element per warp for final aggregation
  int tid = blockIdx.x * blockDim.x + threadIdx.x;
  int lane = threadIdx.x & 31; // lane index in warp
  float val = (tid < N) ? a[tid] : 0.0f; // load (compiler may use reg)
  // warp-level binary tree using shuffles; active mask is full warp
  for (int offset = 16; offset > 0; offset >>= 1) {
    val += __shfl_down_sync(0xFFFFFFFFu, val, offset); // warp reduction
  }
  if (lane == 0) s[threadIdx.x >> 5] = val; // one value per warp
  __syncthreads();
  // first warp aggregates warp results
  if ((threadIdx.x >> 5) == 0) {
    float v = (threadIdx.x < (blockDim.x>>5)) ? s[lane] : 0.0f;
    for (int offset = 16; offset > 0; offset >>= 1) {
      v += __shfl_down_sync(0xFFFFFFFFu, v, offset);
    }
    if (lane == 0) atomicAdd(out, v); // accumulate across blocks
  }
}