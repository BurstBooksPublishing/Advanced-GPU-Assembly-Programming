#include 

// SplitMix64: fast seeding mix (inlineable).
__device__ uint64_t splitmix64(uint64_t *state) {
  uint64_t z = (*state += 0x9E3779B97f4A7C15ULL);
  z = (z ^ (z >> 30)) * 0xBF58476D1CE4E5B9ULL;
  z = (z ^ (z >> 27)) * 0x94D049BB133111EBULL;
  return z ^ (z >> 31);
}

// rotate left for 64-bit
__device__ static inline uint64_t rol64(uint64_t x, int k) {
  return (x << k) | (x >> (64 - k));
}

// xoroshiro128+ next() -- returns output and updates s0,s1 by reference.
__device__ uint64_t xoroshiro128plus_next(uint64_t *s0, uint64_t *s1) {
  uint64_t x = *s0;
  uint64_t y = *s1;
  uint64_t result = x + y;            // output
  y ^= x;
  *s0 = rol64(x, 55) ^ y ^ (y << 14); // new s0
  *s1 = rol64(y, 36);                 // new s1
  return result;
}

// Kernel example: generate M random floats per thread and write to output.
extern "C" __global__ void rng_kernel(uint64_t seed, float *out, size_t M) {
  // derive a unique 64-bit index per thread (linearized)
  uint64_t tid = (uint64_t)blockIdx.x * blockDim.x + threadIdx.x;
  // seed expansion using SplitMix64 (deterministic per tid)
  uint64_t sm_state = seed + tid;
  uint64_t s0 = splitmix64(&sm_state); // initial state part 1
  uint64_t s1 = splitmix64(&sm_state); // initial state part 2

  size_t base = (size_t)tid * M;
  for (size_t i = 0; i < M; ++i) {
    uint64_t r = xoroshiro128plus_next(&s0, &s1); // state stays in registers
    uint32_t r32 = (uint32_t)(r >> 32);           // use high bits for float
    out[base + i] = (float)r32 * 2.3283064e-10f;  // r32 / 2^32
  }
}